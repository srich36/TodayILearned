# Topics

- Inference stacks 
  - Performance/round-trip latency with agent tasks, etc.
- OpenAI Function calling alternatives
  - Any other structured grammars for what LLMs return?
- Function calling -- when you have lots of functions a model can use? This takes up context window
- Context window length -- this is **expensive** and slow
  - In production is everyone running fine-tuned models?
- State of the art RAG pipelines
  - How are people doing this nowadays? 
- Multi-agent systems
  - How is this deployed? What's state of the art here?
- Open vs. closed models -- any to watch?
- How do people deploy models as APIs? Any particularly common way?
- General
  - Multi-modal
  - Where things are heading?

## Overview

- Standard workflow
  - First intent classification step -- should not pass more than 10-20 functions to the LLM
    - For intent classification come up with a set of user queries and classification as few-shot context
    - 2-stage -- if you have 25 things that you can classify as, break down into classifications and subclassifications
      - Can do this directly with the model and then subclassifications -- can get better examples with 2-shot prompting
        - Have additional latency and failure modes
      - In robust systems you want to have a self-checking mechanism, or it might not fit the function
      - For financial services, you'll want to use Azure OpenAI or use an Open source model
- Do people use OpenAI or fine-tune models?
  - It really depends on what the company cares about -- physical latency, etc.
  - 3.5 performance will be substantially worse than 4 without substantial prompt engineer
    - Will want to use some testing suite like weights & biases to evaluate performance
  - This use case is relatively constrained, could use a 7b model and fine-tune it
    - Constrained: general chat bot could be asking it to write code, or you could ask write me a poem
      - In this case you're just asking it financial things about Cardless, this is a very limited set of interactions
    - **Can definitely get better with fine-tuned models, this is mostly what goes on in production**
      - Get data, then fine-tuned models
- Getting fine-tuned data
  1. Hand crafting data
  2. Using a high-performance model to generate the examples, can make templates, etc., to constrain distribution
  3. Use beta users
  - Can use other language models to evaluate and check the models, use humans to comb through them, use reinforcement approach
  - Rule of thumb:
    - Bare minimum is about 1000 examples, 10000
- Intent classification - Statefullness
  - Bespoke state stored in databases, send custom system prompts 
- Prompt engineering -- how to iterate quickly?
  - Break up the chat into subproblems as much as possible
    - Try to solve all the individual subproblems
  - Can **autogenerate a bunch of these pathways**
  - Generate examples with gpt-4 with conversations that are constrained by these
    - GPT-4 or other models can assess the relevance 
- Function calling / tools 
  - A decent number of people use the Falcon model for tools
    - The Falcon models are way bigger
  - For smaller models you have to do a lot more output parsing, regex matching
  - Langchain tools
    - Define a function for the tool, add a decorator to the function, define a list of tools that your model has access to
    - Runtime calls that function, Langchain tries to do output parsing and then calls that function
- State of the art RAG
  - Depending on the embeddings, the way that questions vs. answers are embedded need to be similar to the answers
  - King - man + woman = Queen
    - Using bag or words which is an old technique, which uses sparse vectors
    - Sparse vectors are very easy to do searches but aren't as expressive
  - Vector search today uses models that are trained to do embedding or some other purpose
  - Vector search is an NP hard problem so you have to do some approximation
  - What is mathematically hard?
    - Finding the most similar vectors when you have millions or billions of vectors -- you have to approximate
    - There are ways to approximate this, that's why vector DB companies are so popular these days
    - Hierarchical navigable small worlds is the algorithm used most commonly these days
  - What do these vectors actually mean?
    - The vectors depend on the model used to generate these embeddings
    - It goes through a bunch of layers of dense neural networks, to generate an intermediate representation of the text in a numbers
    - Could fine-tune embedding models as well
      - For advanced use cases (medical applications, med-BERT) this matters
      - For basic use cases the standard embedding model matters
    - Question and answers don't have to represented 
  - Could generate some expected answers to a question 
- Deploying this as an API
  - If you want users to interact via a terminal or embedded in your product or accessible in a web interface
  - Jacob isn't an expert here
  - Using Python is the easy way to go, FastAPI
    - Can connect this to streamlit
- "Multi-agent" systems
  - AutoGPT was the first tool to do this
    - It's changed a lot since doing this
  - Nowadays people are doing multi-agent things like building software developer agencies (e.g. Python agent, GitHub agent, Controller agent, etc.)
    - Each agent can specialize in a different thing, "agents" are autonomous to some degree
    - Seems like an agent marketplace -- first where you can have an agent as an employee 
- An "agent" isn't really what the Cardless demo did -- agents are typically autonomous but aren't responding 
- Multi-modal models
  - One approach
    - One model that can take in inputs of different modalities and work with them
      - CLIP (contrasted image pre-training, used for Dall-E)
        - Can embed the data for an image or embed the data in the text in the same space
  - LLM aided visual reasoning -- the model can use other models that do speech-to-text, vision, 
- Jacob is more on the vision side
  - Has been in Langchain 
- Within the 6 months, 12 months, building on-top of these models will look relatively the same, 2 year 5 year will be very different
  - 3 years, 5 years prompt engineering, RAG won't be there
  - Dall-E uses auto prompting, there was a self-RAG papers
  - Agents -- we are at the very beginning, agents are in their infancy 
    - The true architectural advantages of agents are yet to come
    - The paradigm of having agents interconnected to each other that can grow and adapt and learn together will get us closer to AGI
      - We can't just get there with LLMs, incremental advances will only get us so far