# Overview

- [Paper link](https://arxiv.org/pdf/2311.12908.pdf)
- LLMs are fine-tuned using RLHF for alignment
  - This has not been widely explored in text-to-image models
- DPO was recently formulated as a simpler alternative to RLHF
  - The policy 